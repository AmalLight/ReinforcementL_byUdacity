"""
    Blackjack is a card game where the goal is to obtain cards that sum to as
    near as possible to 21 without going over.
    They're playing against a fixed dealer.
    Face cards (Jack, Queen, King) have point value 10.
    Aces can either count as 11 or 1, and it's called 'usable' at 11.

    This game is placed with an infinite deck (or with replacement).
    The game starts with each (player and dealer) having one face up and one
    face down card.

    The player can request additional cards (hit=1) until they decide to stop
    (stick=0) or exceed 21 (bust).

    After the player sticks, the dealer reveals their facedown card, and draws
    until their sum is 17 or greater.
    If the dealer goes bust the player wins.

    If neither player nor dealer busts, the outcome (win, lose, draw) is
    decided by whose sum is closer to 21. 
    The reward for winning is +1, drawing is 0, and losing is -1.

    The observation of a 3-tuple of: the players current sum,
    the dealer's one showing card (1-10 where 1 is ace),
    and whether or not the player holds a usable ace (0 or 1).
"""

# load libs
# load env <-- unity?
# create a fast collection of episodes like a Demo

# function
#   generate_episode(s)_from_limit_stochastic <- from AI with probability
#      np.arange(3) >> array([0, 1, 2])
#      np.random.choice array p=X% >> 0 or 1 or 2
#         pseudocode haskell style
#      result1 >> [ ((13, 10, False), 1, -1)] > 21
#      result2 >> [ ((15,  2, False), 1,  0), ((19, 2, False), 0, 1.0)] = 21
#      result3 >> [ ((12, 10, False), 1, -1)] > 21
#      result4 >> [ ((18, 10, True ), 1,  0), ((19, 10, True ), 1,  0),
#                   ((13, 10, False), 1,  0), ((20, 10, False), 1, -1)]
#   for now none consideration about the state

# function
#   mc_prediction_q_table(env, num_episodes, generate_episode, gamma=1.0)
#      gamma: This is the discount rate = [0,1]

# iteration
#   show it/they ( the result(s) ).
#   maybe I could say it is the mindset of our AI

# function
#   mc_control <- get the default policies
#      obtain the corresponding state-value function

# iteration
#   plot the policy
